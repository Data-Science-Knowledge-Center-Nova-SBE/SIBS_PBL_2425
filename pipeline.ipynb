{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fraud Business Impact Forecasting: NOVA x SIBS Pipeline\n",
        "\n",
        "This notebook processes transaction data to create features for fraud amount prediction.\n",
        "The pipeline consists of three main phases:\n",
        "\n",
        "1. **Spark-based feature engineering**\n",
        "2. **Data transformation and aggregation** \n",
        "3. **Machine learning model training and evaluation**\n",
        "\n",
        "## Table of Contents\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Data Loading Functions](#data-loading)\n",
        "3. [Feature Engineering Functions](#feature-engineering)\n",
        "4. [Data Processing Functions](#data-processing)\n",
        "5. [Modeling Functions](#modeling)\n",
        "6. [Pipeline Execution](#execution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "import pandas as pd\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, avg, when, sin, cos, lit, to_date, date_sub, sum as Fsum\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "import shap\n",
        "\n",
        "# Configure matplotlib for inline plots\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"All imports completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading Functions\n",
        "\n",
        "These functions handle Spark session initialization and data loading from the database tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_spark_session(session_name=\"fraud_pipeline\"):\n",
        "    \"\"\"\n",
        "    Initialize Spark session with optimized configurations for the fraud detection pipeline.\n",
        "    \n",
        "    Args:\n",
        "        session_name (str): Name for the Spark session\n",
        "        \n",
        "    Returns:\n",
        "        SparkSession: Configured Spark session\n",
        "    \"\"\"\n",
        "    python_packages_path = \"/home/cdsw/ml_new.tar.gz#environment\"\n",
        "    executor_memory = \"10g\"\n",
        "    driver_memory = \"8g\"\n",
        "    executor_cores = \"3\"\n",
        "    maxExecutors = 15\n",
        "    memory_overhead = \"2g\"\n",
        "    n_partitions = 200\n",
        "    driver_memory_overhead = 8000\n",
        "\n",
        "    spark = SparkSession\\\n",
        "            .builder\\\n",
        "            .config(\"spark.yarn.dist.archives\", python_packages_path)\\\n",
        "            .config(\"spark.executor.memory\", executor_memory)\\\n",
        "            .config(\"spark.driver.memory\", driver_memory)\\\n",
        "            .config(\"spark.executor.cores\", executor_cores)\\\n",
        "            .config(\"spark.yarn.executor.memoryOverhead\", memory_overhead)\\\n",
        "            .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
        "            .config(\"spark.dynamicAllocation.maxExecutors\", maxExecutors)\\\n",
        "            .config(\"spark.sql.shuffle.partitions\", n_partitions)\\\n",
        "            .config(\"spark.sql.adaptive.enabled\", True)\\\n",
        "            .config(\"spark.yarn.driver.memoryOverhead\", driver_memory_overhead)\\\n",
        "            .config(\"spark.yarn.queue\", \"root.nova_sbe\")\\\n",
        "            .appName(session_name)\\\n",
        "            .getOrCreate()\n",
        "    \n",
        "    return spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(spark):\n",
        "    \"\"\"\n",
        "    Load training, out-of-sample, and card status data from Spark tables.\n",
        "    \n",
        "    Args:\n",
        "        spark (SparkSession): Active Spark session\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (training_df, oos_df, card_status_full_df) - loaded DataFrames\n",
        "    \"\"\"\n",
        "    # Load training and out-of-sample datasets\n",
        "    training = spark.table(\"nova_sbe.raw_202306_202412\")\n",
        "    card_status = spark.table(\"nova_sbe.card_status_202306_202412\")\n",
        "    oos = spark.table(\"nova_sbe.sample_202412\")\n",
        "    card_status_outsample = spark.table(\"nova_sbe.card_status_202501_202504\")\n",
        "    \n",
        "    # Ensure out-of-sample has same columns as training\n",
        "    oos = oos[training.columns]\n",
        "    \n",
        "    # Concatenate card status DataFrames\n",
        "    card_status_full = card_status.union(card_status_outsample)\n",
        "    \n",
        "    return training, oos, card_status_full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering Functions\n",
        "\n",
        "These functions create various types of features including target variables, time features, \n",
        "rolling statistics, merchant features, and card features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_target_variable(df, training_df):\n",
        "    \"\"\"\n",
        "    Create fraud-related target variables and labels.\n",
        "    \n",
        "    Args:\n",
        "        df (DataFrame): Main DataFrame to add target variables to\n",
        "        training_df (DataFrame): Training DataFrame for creating fraud labels\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: DataFrame with added target variables\n",
        "    \"\"\"\n",
        "    # Create fraud label in training dataset to avoid data leakage\n",
        "    training_df = training_df.withColumn(\n",
        "        \"fraud_label\",\n",
        "        F.when(F.col('fraud_sp') > 0, 1)  # Confirmed fraud\n",
        "        .when(F.col('fraud_sp') <= 0, -1)  # Confirmed genuine\n",
        "        .otherwise(0)  # Unknown\n",
        "    )\n",
        "    \n",
        "    # Create fraud label in main dataset\n",
        "    df = df.withColumn(\n",
        "        \"fraud_label\",\n",
        "        F.when(F.col('fraud_sp') > 0, 1)\n",
        "        .when(F.col('fraud_sp') <= 0, -1)\n",
        "        .otherwise(0)\n",
        "    )\n",
        "    \n",
        "    # Create fraud amount (amount when transaction is fraud)\n",
        "    df = df.withColumn(\n",
        "        \"fraud_amount\",\n",
        "        when(col(\"fraud_label\") == 1, col(\"OPERATION_AMOUNT\")).otherwise(0)\n",
        "    )\n",
        "    \n",
        "    # Create transaction accepted flag\n",
        "    df = df.withColumn(\n",
        "        \"TRANSACTION_ACCEPTED\",\n",
        "        F.when(F.col(\"RESPONSE_STATUS_CDE\") == \"ACCP\", 1).otherwise(0)\n",
        "    )\n",
        "    \n",
        "    # Create fraud amount accepted (target variable)\n",
        "    df = df.withColumn(\n",
        "        \"fraud_amount_accepted\",\n",
        "        when(col(\"TRANSACTION_ACCEPTED\") == 1, col(\"fraud_amount\")).otherwise(0)\n",
        "    )\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_time_features(df):\n",
        "    \"\"\"\n",
        "    Create time-based features including cyclical transformations.\n",
        "    \n",
        "    Args:\n",
        "        df (DataFrame): Input DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: DataFrame with added time features\n",
        "    \"\"\"\n",
        "    # Extract basic time components\n",
        "    df = df.withColumn(\"TRANSACTION_MONTH\", F.month(F.col(\"TRANSACTION_DATETIME\")))\\\n",
        "           .withColumn(\"HOUR_OF_DAY\", F.hour(F.col(\"TRANSACTION_DATETIME\")))\\\n",
        "           .withColumn(\"DAY_OF_WEEK\", F.dayofweek(F.col(\"TRANSACTION_DATETIME\")))\\\n",
        "           .withColumn(\"DAY_OF_MONTH\", F.dayofmonth(F.col(\"TRANSACTION_DATETIME\")))\\\n",
        "           .withColumn(\"TRANSACTION_DATE\", F.to_date(\"TRANSACTION_DATETIME\"))\n",
        "    \n",
        "    # Sin/Cosine transformations for cyclical features\n",
        "    max_day = 31\n",
        "    max_month = 12\n",
        "    pi = 3.141592653589793\n",
        "    \n",
        "    # Day of month cyclical transformation\n",
        "    df = df.withColumn(\"dom_angle\", 2 * lit(pi) * col(\"DAY_OF_MONTH\") / lit(max_day))\\\n",
        "           .withColumn(\"dom_sin\", sin(col(\"dom_angle\")))\\\n",
        "           .withColumn(\"dom_cos\", cos(col(\"dom_angle\")))\\\n",
        "           .drop(\"dom_angle\")\n",
        "    \n",
        "    # Month cyclical transformation\n",
        "    df = df.withColumn(\"month_angle\", 2 * lit(pi) * col(\"TRANSACTION_MONTH\") / lit(max_month))\\\n",
        "           .withColumn(\"month_sin\", sin(col(\"month_angle\")))\\\n",
        "           .withColumn(\"month_cos\", cos(col(\"month_angle\")))\\\n",
        "           .drop(\"month_angle\")\n",
        "    \n",
        "    # Weekend indicator\n",
        "    df = df.withColumn(\"is_weekend\", F.when(F.dayofweek(\"TRANSACTION_DATE\").isin(1, 7), 1).otherwise(0))\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rolling_features(df):\n",
        "    \"\"\"\n",
        "    Create rolling statistics and lag features for fraud amounts.\n",
        "    \n",
        "    Args:\n",
        "        df (DataFrame): Input DataFrame with date columns\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (df, rolling_stats_7, rolling_stats_14, lag1_df, lag7_df, lag14_df)\n",
        "    \"\"\"\n",
        "    # Ensure date columns exist\n",
        "    df = df.withColumn(\"day\", F.to_date(\"TRANSACTION_DATETIME\"))\\\n",
        "           .withColumn(\"classification_day\", F.to_date(\"fraud_classification_datetime\"))\n",
        "    \n",
        "    # Prepare data for rolling calculations\n",
        "    df_valid = df.select(\"day\", \"classification_day\", \"fraud_amount_accepted\")\n",
        "    target_days = df.select(\"day\").distinct().withColumnRenamed(\"day\", \"target_day\")\n",
        "    \n",
        "    # Cross join and filter for valid combinations\n",
        "    joined = df_valid.crossJoin(target_days)\\\n",
        "        .filter(F.col(\"day\") < F.col(\"target_day\"))\\\n",
        "        .filter(F.col(\"classification_day\") <= F.col(\"target_day\"))\n",
        "    \n",
        "    # Aggregate daily fraud amounts\n",
        "    daily_agg_per_target = joined.groupBy(\"target_day\", \"day\").agg(\n",
        "        F.sum(\"fraud_amount_accepted\").alias(\"daily_fraud_sum\")\n",
        "    )\n",
        "    \n",
        "    # 7-day rolling statistics\n",
        "    rolling_base_7 = daily_agg_per_target\\\n",
        "        .withColumn(\"days_diff\", F.datediff(F.col(\"target_day\"), F.col(\"day\")))\\\n",
        "        .filter(F.col(\"days_diff\").between(1, 7))\n",
        "    \n",
        "    rolling_stats_7 = rolling_base_7.groupBy(\"target_day\").agg(\n",
        "        F.mean(\"daily_fraud_sum\").alias(\"rolling_mean_7\"),\n",
        "        F.max(\"daily_fraud_sum\").alias(\"rolling_max_7\"),\n",
        "        F.min(\"daily_fraud_sum\").alias(\"rolling_min_7\"),\n",
        "        F.sum(\"daily_fraud_sum\").alias(\"rolling_sum_7\"),\n",
        "        F.stddev(\"daily_fraud_sum\").alias(\"rolling_std_7\")\n",
        "    ).orderBy(\"target_day\")\n",
        "    \n",
        "    # 14-day rolling statistics\n",
        "    rolling_base_14 = daily_agg_per_target\\\n",
        "        .withColumn(\"days_diff\", F.datediff(F.col(\"target_day\"), F.col(\"day\")))\\\n",
        "        .filter(F.col(\"days_diff\").between(1, 14))\n",
        "    \n",
        "    rolling_stats_14 = rolling_base_14.groupBy(\"target_day\").agg(\n",
        "        F.mean(\"daily_fraud_sum\").alias(\"rolling_mean_14\"),\n",
        "        F.max(\"daily_fraud_sum\").alias(\"rolling_max_14\"),\n",
        "        F.min(\"daily_fraud_sum\").alias(\"rolling_min_14\"),\n",
        "        F.sum(\"daily_fraud_sum\").alias(\"rolling_sum_14\"),\n",
        "        F.stddev(\"daily_fraud_sum\").alias(\"rolling_std_14\")\n",
        "    ).orderBy(\"target_day\")\n",
        "    \n",
        "    # Create lag features\n",
        "    target_days = df.select(\"day\").distinct().withColumnRenamed(\"day\", \"target_day\")\n",
        "    \n",
        "    # 1-day lag\n",
        "    df_lag1 = df.crossJoin(target_days)\\\n",
        "        .filter(col(\"day\") == date_sub(col(\"target_day\"), 1))\\\n",
        "        .filter(col(\"classification_day\") <= col(\"target_day\"))\n",
        "    lag1_df = df_lag1.groupBy(\"target_day\").agg(Fsum(\"fraud_amount_accepted\").alias(\"lag_1\"))\n",
        "    \n",
        "    # 7-day lag\n",
        "    df_lag7 = df.crossJoin(target_days)\\\n",
        "        .filter(col(\"day\") == date_sub(col(\"target_day\"), 7))\\\n",
        "        .filter(col(\"classification_day\") <= col(\"target_day\"))\n",
        "    lag7_df = df_lag7.groupBy(\"target_day\").agg(Fsum(\"fraud_amount_accepted\").alias(\"lag_7\"))\n",
        "    \n",
        "    # 14-day lag\n",
        "    df_lag14 = df.crossJoin(target_days)\\\n",
        "        .filter(col(\"day\") == date_sub(col(\"target_day\"), 14))\\\n",
        "        .filter(col(\"classification_day\") <= col(\"target_day\"))\n",
        "    lag14_df = df_lag14.groupBy(\"target_day\").agg(Fsum(\"fraud_amount_accepted\").alias(\"lag_14\"))\n",
        "    \n",
        "    return df, rolling_stats_7, rolling_stats_14, lag1_df, lag7_df, lag14_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_merchant_features(df, training_df):\n",
        "    \"\"\"\n",
        "    Create merchant-related features including risk assessment and categorization.\n",
        "    \n",
        "    Args:\n",
        "        df (DataFrame): Main DataFrame\n",
        "        training_df (DataFrame): Training DataFrame for risk calculations\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: DataFrame with added merchant features\n",
        "    \"\"\"\n",
        "    # Merchant frequency and new merchant detection\n",
        "    window_spec = Window.partitionBy(\"MERCHANT_NAME\").orderBy(\"TRANSACTION_DATE\")\n",
        "    df = df.withColumn(\"first_seen_date\", F.min(\"TRANSACTION_DATE\").over(window_spec))\n",
        "    df = df.withColumn(\"is_new_merchant\", (F.col(\"TRANSACTION_DATE\") == F.col(\"first_seen_date\")).cast(\"int\"))\n",
        "    \n",
        "    # Merchant size categorization (small vs big merchants)\n",
        "    merchant_freq_df = df.groupBy(\"MERCHANT_NAME\").agg(F.count(\"*\").alias(\"tx_count\"))\n",
        "    window_spec = Window.orderBy(\"tx_count\")\n",
        "    merchant_ranked_df = merchant_freq_df.withColumn(\"percent_rank\", F.percent_rank().over(window_spec))\n",
        "    \n",
        "    merchant_labeled_df = merchant_ranked_df.withColumn(\n",
        "        \"small_merchant\", F.when(F.col(\"percent_rank\") <= 0.05, 1).otherwise(0)\n",
        "    ).withColumn(\n",
        "        \"big_merchant\", F.when(F.col(\"percent_rank\") > 0.05, 1).otherwise(0)\n",
        "    ).select(\"MERCHANT_NAME\", \"small_merchant\", \"big_merchant\")\n",
        "    \n",
        "    df = df.join(merchant_labeled_df, on=\"MERCHANT_NAME\", how=\"left\")\n",
        "    \n",
        "    # MCC fraud risk ranking\n",
        "    mcc_fraud_pct = training_df.groupBy(\"MCC\").agg(\n",
        "        F.round(100 * F.count(F.when(F.col('fraud_label') == 1, 1)) / F.count('*'), 4).alias('mcc_fraud_rate')\n",
        "    )\n",
        "    \n",
        "    window_spec = Window.orderBy(F.col(\"mcc_fraud_rate\").asc())\n",
        "    ranked_mcc = mcc_fraud_pct.withColumn(\"fraud_rank\", F.dense_rank().over(window_spec))\n",
        "    ranked_mcc_df_filtered = ranked_mcc.select(\"MCC\", \"fraud_rank\")\n",
        "    \n",
        "    df = df.join(ranked_mcc_df_filtered, on=\"MCC\", how=\"left\")\n",
        "    df = df.withColumnRenamed(\"fraud_rank\", \"fraud_risk_rank\")\n",
        "    \n",
        "    # Penalized operation amount based on risk\n",
        "    df = df.withColumn(\"penalized_operation_amount\", F.col(\"OPERATION_AMOUNT\") * F.col(\"fraud_risk_rank\"))\n",
        "    \n",
        "    # High-risk merchant identification\n",
        "    fraud_threshold = mcc_fraud_pct.approxQuantile(\"mcc_fraud_rate\", [0.95], 0.01)[0]\n",
        "    high_risk_mcc = mcc_fraud_pct.withColumn(\n",
        "        \"is_high_risk_merchant\", F.when(F.col(\"mcc_fraud_rate\") >= fraud_threshold, 1).otherwise(0)\n",
        "    ).select(\"MCC\", \"is_high_risk_merchant\")\n",
        "    \n",
        "    df = df.join(high_risk_mcc, on=\"MCC\", how=\"left\").fillna({\"is_high_risk_merchant\": 0})\n",
        "    \n",
        "    # Merchant name processing\n",
        "    df = df.withColumn(\"MERCHANT_NAME_LOWER\", F.lower(F.col(\"MERCHANT_NAME\")))\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_card_features(df, card_status_full):\n",
        "    \"\"\"\n",
        "    Create card-related features including authorization response codes and cancellations.\n",
        "    \n",
        "    Args:\n",
        "        df (DataFrame): Main DataFrame\n",
        "        card_status_full (DataFrame): Card status DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: DataFrame with added card features\n",
        "    \"\"\"\n",
        "    # Authorization response code features\n",
        "    df = df.withColumn(\"number_tx_exceeded\", F.when(F.col(\"AUTHORISATION_RESPONSE_CDE\") == 65, 1).otherwise(0))\n",
        "    df = df.withColumn(\"cash_withdrawal_exceeded\", F.when(F.col(\"AUTHORISATION_RESPONSE_CDE\") == 61, 1).otherwise(0))\n",
        "    df = df.withColumn(\"card_number_invalid\", F.when(F.col(\"AUTHORISATION_RESPONSE_CDE\") == 14, 1).otherwise(0))\n",
        "    df = df.withColumn(\"invalid_pin\", F.when(F.col(\"AUTHORISATION_RESPONSE_CDE\") == 55, 1).otherwise(0))\n",
        "    df = df.withColumn(\"invalid_cvv\", F.when(F.col(\"AUTHORISATION_RESPONSE_CDE\") == 82, 1).otherwise(0))\n",
        "    \n",
        "    # Card cancellations\n",
        "    card_status_full = card_status_full.withColumn(\"TRANSACTION_DATE\", to_date(col(\"TRANSACTION_DATETIME\")))\n",
        "    card_cancellations = card_status_full.filter(col(\"NEW_CARD_STATUS\").isin(\"06\", \"09\"))\n",
        "    daily_cancellations = card_cancellations.groupBy(\"TRANSACTION_DATE\").count().withColumnRenamed(\"count\", \"DAILY_CANCELLED_CARD_COUNT\")\n",
        "    daily_cancellations = daily_cancellations.withColumnRenamed(\"DAILY_CANCELLED_CARD_COUNT\", \"DAILY_CANCELLED_CARD_COUNT_cancellations\")\n",
        "    \n",
        "    df = df.join(daily_cancellations, on=\"TRANSACTION_DATE\", how=\"left\")\n",
        "    \n",
        "    # Remote transaction indicator\n",
        "    df = df.withColumn(\"is_remote\", F.col(\"FRAUD_ACCEPTOR_CDE\").startswith(\"REMOTE\").cast(\"int\"))\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Processing Functions\n",
        "\n",
        "These functions handle data aggregation, conversion between Spark and Pandas, and target variable correction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def aggregate_daily_features(df):\n",
        "    \"\"\"\n",
        "    Aggregate all features on a daily basis.\n",
        "    \n",
        "    Args:\n",
        "        df (DataFrame): DataFrame with all features\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: Daily aggregated DataFrame\n",
        "    \"\"\"\n",
        "    daily_data = df.groupBy(\"TRANSACTION_DATE\", \"is_weekend\").agg(\n",
        "        F.sum(F.when(df[\"small_merchant\"] == 1, F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"small_merchant_amount\"),\n",
        "        F.sum(F.when(df[\"big_merchant\"] == 1, F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"big_merchant_amount\"),\n",
        "        F.sum(\"penalized_operation_amount\").alias(\"penalized_total_operation_amount\"),\n",
        "        F.round(F.sum(\"penalized_operation_amount\") / F.countDistinct(\"PAN_ENCRYPTED\"), 4).alias('avg_pen_amount_per_card'),\n",
        "        F.round(F.sum(\"penalized_operation_amount\") / F.sum(\"OPERATION_AMOUNT\"), 4).alias('avg_pen_amount_norm'),\n",
        "        F.sum(\"DAILY_CANCELLED_CARD_COUNT_cancellations\").alias(\"total_cancellations\"),\n",
        "        F.count(\"*\").alias(\"total_tx\"),\n",
        "        F.sum(\"is_remote\").alias(\"remote_tx\"),\n",
        "        F.countDistinct(\"PAN_ENCRYPTED\").alias(\"unique_cards\"),\n",
        "        F.countDistinct(\"MERCHANT_NAME\").alias(\"unique_merchants\"),\n",
        "        F.countDistinct(\"MCC\").alias(\"mcc_distinct_counts\"),\n",
        "        F.round(F.sum(\"OPERATION_AMOUNT\") / F.countDistinct(\"PAN_ENCRYPTED\"), 4).alias(\"avg_unique_card_amount\"),\n",
        "        F.sum(F.when(F.col(\"RESPONSE_STATUS_CDE\") == \"RJCT\", 1).otherwise(0)).alias(\"total_rejected\"),\n",
        "        F.sum(F.when(F.col(\"RESPONSE_STATUS_CDE\") == \"ACCP\", 1).otherwise(0)).alias(\"total_accepted\"),\n",
        "        F.round(F.sum(F.when(F.col(\"RESPONSE_STATUS_CDE\") == \"RJCT\", 1).otherwise(0)) / F.countDistinct(\"MERCHANT_NAME\"), 4).alias(\"avg_rejected_tx_merchant\"),\n",
        "        F.sum(\"fraud_amount_accepted\").alias(\"fraud_amount_accepted\"),\n",
        "        F.sum(\"OPERATION_AMOUNT\").alias(\"total_operation_amount\"),\n",
        "        F.first(\"month_sin\").alias(\"month_sin\"),\n",
        "        F.first(\"month_cos\").alias(\"month_cos\"),\n",
        "        F.first(\"dom_cos\").alias(\"day_of_month_cos\"),\n",
        "        F.first(\"dom_sin\").alias(\"day_of_month_sin\"),\n",
        "        F.count(F.col(\"number_tx_exceeded\")).alias(\"number_tx_exceeded\"),\n",
        "        F.count(F.col(\"cash_withdrawal_exceeded\")).alias(\"cash_withdrawal_exceeded\"),\n",
        "        F.count(F.col(\"card_number_invalid\")).alias(\"card_number_invalid\"),\n",
        "        F.count(F.col(\"invalid_pin\")).alias(\"invalid_pin\"),\n",
        "        F.count(F.col(\"invalid_cvv\")).alias(\"invalid_cvv\"),\n",
        "        F.countDistinct(F.concat_ws(\"_\", F.col(\"PAN_ENCRYPTED\"), F.col(\"MERCHANT_NAME\"))).alias(\"unique_merchant_card_pairs\"),\n",
        "        F.sum(\"is_new_merchant\").alias(\"new_merchant_tx\"),\n",
        "        F.sum(F.when(F.col(\"is_new_merchant\") == 1, F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"new_merchant_amount\"),\n",
        "        F.round(F.sum('OPERATION_AMOUNT') / F.count('*'), 4).alias('amount_transaction_ratio'),\n",
        "        F.round(F.count('*') / F.countDistinct(\"MERCHANT_NAME\"), 4).alias('avg_transaction_per_merchant'),\n",
        "        F.round(F.sum(F.when(F.col(\"RESPONSE_STATUS_CDE\") == \"RJCT\", 1).otherwise(0)) / F.sum(F.when(F.col(\"RESPONSE_STATUS_CDE\") == \"ACCP\", 1).otherwise(0)), 4).alias('rejected_accepted_ratio'),\n",
        "        F.round(F.sum(\"is_remote\") / F.count('*'), 4).alias('remote_ratio'),\n",
        "        F.round(F.sum(\"is_high_risk_merchant\") / F.count('*'), 4).alias('high_risk_tx_ratio'),\n",
        "        F.sum(F.when(F.col(\"MERCHANT_NAME_LOWER\").contains(\"vinted\"), F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"vinted_amount\"),\n",
        "        F.sum(F.when(F.col(\"MCC\") == \"6051\", 1).otherwise(0)).alias(\"mcc_6051_count\"),\n",
        "        F.sum(F.when(F.col(\"MCC\") == \"4829\", 1).otherwise(0)).alias(\"mcc_4829_count\"),\n",
        "        F.sum(F.when(F.col(\"MCC\") == \"6051\", F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"mcc_6051_amount\"),\n",
        "        F.sum(F.when(F.col(\"MCC\") == \"4829\", F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"mcc_4829_amount\"),\n",
        "        F.sum(F.when(F.col(\"MCC\") == \"7995\", F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"mcc_7995_amount\"),\n",
        "        F.sum(F.when(F.col(\"MCC\") == \"5999\", F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"mcc_5999_amount\"),\n",
        "        F.sum(F.when(F.col(\"MCC\") == \"4722\", F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"mcc_4722_amount\"),\n",
        "        F.sum(F.when(F.col(\"MCC\") == \"5944\", F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"mcc_5944_amount\"),\n",
        "        F.sum(F.when(F.col(\"MERCHANT_NAME\") == \"BETANO PT\", F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"BETANO_PT_amount\"),\n",
        "        F.sum(F.when(F.col(\"MERCHANT_NAME_LOWER\").contains(\"binance.com\"), F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"binance_amount\"),\n",
        "        F.sum(F.when(F.col(\"MERCHANT_NAME_LOWER\").contains(\"bifinity\"), F.col(\"OPERATION_AMOUNT\")).otherwise(0)).alias(\"bifinity_amount\")\n",
        "    ).orderBy(\"TRANSACTION_DATE\", ascending=True)\n",
        "    \n",
        "    return daily_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_pandas(daily_data, rolling_stats_7, rolling_stats_14, lag1_df, lag7_df, lag14_df):\n",
        "    \"\"\"\n",
        "    Convert Spark DataFrames to Pandas and merge all features.\n",
        "    \n",
        "    Args:\n",
        "        daily_data, rolling_stats_7, rolling_stats_14, lag1_df, lag7_df, lag14_df: Spark DataFrames\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Merged Pandas DataFrame with all features\n",
        "    \"\"\"\n",
        "    # Convert to Pandas\n",
        "    daily_pd = daily_data.toPandas()\n",
        "    rolling_stats_7_pd = rolling_stats_7.toPandas()\n",
        "    rolling_stats_14_pd = rolling_stats_14.toPandas()\n",
        "    lag1_df_pd = lag1_df.toPandas()\n",
        "    lag7_df_pd = lag7_df.toPandas()\n",
        "    lag14_df_pd = lag14_df.toPandas()\n",
        "    \n",
        "    # Merge all features\n",
        "    df_pd = daily_pd.merge(rolling_stats_7_pd, left_on=\"TRANSACTION_DATE\", right_on=\"target_day\", how=\"left\").drop(columns=[\"target_day\"])\n",
        "    df_pd = df_pd.merge(rolling_stats_14_pd, left_on=\"TRANSACTION_DATE\", right_on=\"target_day\", how=\"left\").drop(columns=[\"target_day\"])\n",
        "    df_pd = df_pd.merge(lag1_df_pd, left_on=\"TRANSACTION_DATE\", right_on=\"target_day\", how=\"left\").drop(columns=[\"target_day\"])\n",
        "    df_pd = df_pd.merge(lag7_df_pd, left_on=\"TRANSACTION_DATE\", right_on=\"target_day\", how=\"left\").drop(columns=[\"target_day\"])\n",
        "    df_pd = df_pd.merge(lag14_df_pd, left_on=\"TRANSACTION_DATE\", right_on=\"target_day\", how=\"left\").drop(columns=[\"target_day\"])\n",
        "    \n",
        "    # Handle missing values\n",
        "    df_pd = df_pd.bfill()\n",
        "    df_pd = df_pd.sort_values(\"TRANSACTION_DATE\", ascending=True)\n",
        "    df_pd = df_pd.reset_index(drop=True)\n",
        "    \n",
        "    return df_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_corrected_target(df_pd, training_df):\n",
        "    \"\"\"\n",
        "    Create fraud labelling delay corrected target variable.\n",
        "    \n",
        "    Args:\n",
        "        df_pd (pd.DataFrame): Main DataFrame\n",
        "        training_df (DataFrame): Training Spark DataFrame\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with corrected target variable\n",
        "    \"\"\"\n",
        "    # Calculate fraud labelling delay\n",
        "    df_delay = training_df.select(\n",
        "        'id', 'TRANSACTION_DATETIME', 'fraud_classification_datetime', 'fraud_label'\n",
        "    ).filter('fraud_label == 1')\n",
        "    \n",
        "    df_delay = df_delay.withColumn(\n",
        "        \"LABEL_DELAY_DAYS\",\n",
        "        (F.col(\"fraud_classification_datetime\").cast(\"long\") - F.col(\"TRANSACTION_DATETIME\").cast(\"long\")) / 86400\n",
        "    ).withColumn(\n",
        "        \"delay_days_rounded\",\n",
        "        F.floor(F.col('LABEL_DELAY_DAYS'))\n",
        "    )\n",
        "    \n",
        "    # Calculate labelling curve\n",
        "    counts_by_day = df_delay.groupBy('delay_days_rounded').agg(\n",
        "        F.count('*').alias('labelled_count')\n",
        "    ).orderBy('delay_days_rounded')\n",
        "    \n",
        "    total_transactions = df_delay.count()\n",
        "    w = Window.orderBy('delay_days_rounded').rowsBetween(Window.unboundedPreceding, 0)\n",
        "    \n",
        "    counts_by_day = counts_by_day.withColumn(\n",
        "        'cummulative_labelled', F.sum('labelled_count').over(w)\n",
        "    ).withColumn(\n",
        "        'pct_labelled', (F.col('cummulative_labelled') / F.lit(total_transactions)) * 100\n",
        "    )\n",
        "    \n",
        "    label_curve = counts_by_day.select('delay_days_rounded', 'pct_labelled').filter('delay_days_rounded >= 0')\n",
        "    label_curve_pd = label_curve.toPandas()\n",
        "    \n",
        "    # Apply correction\n",
        "    labeling_curve_dict = dict(zip(label_curve_pd['delay_days_rounded'], (label_curve_pd['pct_labelled']) / 100))\n",
        "    dataset_given_date = pd.Timestamp('2025-06-06')\n",
        "    \n",
        "    df_pd['days_since_transaction'] = (dataset_given_date - pd.to_datetime(df_pd['TRANSACTION_DATE'])).dt.days\n",
        "    max_known_day = max(labeling_curve_dict.keys())\n",
        "    df_pd['days_since_transaction'] = df_pd['days_since_transaction'].clip(upper=max_known_day)\n",
        "    \n",
        "    df_pd['correction_factor'] = df_pd['days_since_transaction'].map(labeling_curve_dict)\n",
        "    df_pd['corrected_accepted_fraud_amount'] = ((df_pd['fraud_amount_accepted'] / 100) / df_pd['correction_factor']) * 100\n",
        "    \n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df_pd['TRANSACTION_DATE'], df_pd['corrected_accepted_fraud_amount'], label='Corrected', color='red')\n",
        "    plt.plot(df_pd['TRANSACTION_DATE'], df_pd['fraud_amount_accepted'], label='Actual')\n",
        "    plt.title(\"Accepted Fraud Amount: Actual vs Corrected\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Amount\")\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return df_pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Modeling Functions\n",
        "\n",
        "These functions handle data preparation for modeling, feature selection, model training, and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_modeling_data(df_pd):\n",
        "    \"\"\"\n",
        "    Prepare data for modeling by creating training and test splits.\n",
        "    \n",
        "    Args:\n",
        "        df_pd (pd.DataFrame): Complete DataFrame with all features\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (train_data, test_data) - training and testing DataFrames\n",
        "    \"\"\"\n",
        "    df_pd['TRANSACTION_DATE'] = pd.to_datetime(df_pd['TRANSACTION_DATE'])\n",
        "    \n",
        "    train_data = df_pd[(df_pd['TRANSACTION_DATE'] >= '2023-06-01') & (df_pd['TRANSACTION_DATE'] <= '2024-12-31')]\n",
        "    test_data = df_pd[(df_pd['TRANSACTION_DATE'] >= '2025-01-01') & (df_pd['TRANSACTION_DATE'] <= '2025-04-30')]\n",
        "    \n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Test data shape: {test_data.shape}\")\n",
        "    \n",
        "    return train_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def feature_selection_analysis(train_data):\n",
        "    \"\"\"\n",
        "    Perform feature selection analysis using Mutual Information.\n",
        "    \n",
        "    Args:\n",
        "        train_data (pd.DataFrame): Training data\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with MI scores for features\n",
        "    \"\"\"\n",
        "    # Mutual Information Analysis\n",
        "    X = train_data.drop(columns=['TRANSACTION_DATE', 'fraud_amount_accepted', 'days_since_transaction', 'correction_factor', 'corrected_accepted_fraud_amount'], axis=1).fillna(0)\n",
        "    y = train_data['fraud_amount_accepted']\n",
        "    \n",
        "    mi_scores = mutual_info_regression(X, y)\n",
        "    mi_df = pd.DataFrame({'Feature': X.columns, 'MI_Score': mi_scores}).sort_values(by='MI_Score', ascending=False)\n",
        "    \n",
        "    print(\"Top 15 Features by Mutual Information Score:\")\n",
        "    print(mi_df.head(15))\n",
        "    \n",
        "    # Plot top features\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = mi_df.head(15)\n",
        "    plt.barh(top_features['Feature'], top_features['MI_Score'])\n",
        "    plt.xlabel('Mutual Information Score')\n",
        "    plt.title('Top 15 Features by Mutual Information Score')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return mi_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Train Random Forest model and evaluate performance.\n",
        "    \n",
        "    Args:\n",
        "        train_data (pd.DataFrame): Training data\n",
        "        test_data (pd.DataFrame): Testing data\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (best_model, predictions, test_dates) - trained model, predictions, and test dates\n",
        "    \"\"\"\n",
        "    # Prepare data\n",
        "    train_dates = train_data[\"TRANSACTION_DATE\"]\n",
        "    test_dates = test_data[\"TRANSACTION_DATE\"]\n",
        "    \n",
        "    df_train = train_data.drop(columns=[\"TRANSACTION_DATE\"])\n",
        "    df_test = test_data.drop(columns=[\"TRANSACTION_DATE\"]).bfill().fillna(0)\n",
        "    \n",
        "    # Manual oversampling for imbalanced data\n",
        "    threshold = 100000\n",
        "    peak_days = df_train[df_train[\"corrected_accepted_fraud_amount\"] > threshold]\n",
        "    normal_days = df_train[df_train[\"corrected_accepted_fraud_amount\"] <= threshold]\n",
        "    df_train_balanced = pd.concat([normal_days, pd.concat([peak_days] * 5)], ignore_index=True)\n",
        "    \n",
        "    print(f\"Original training size: {len(df_train)}\")\n",
        "    print(f\"Balanced training size: {len(df_train_balanced)}\")\n",
        "    \n",
        "    # Feature selection based on analysis\n",
        "    features = ['unique_cards', 'total_rejected', 'total_accepted', 'mcc_4829_count',\n",
        "                'total_operation_amount', 'total_tx', 'remote_tx', 'unique_merchant_card_pairs',\n",
        "                'new_merchant_tx', 'total_cancellations', 'month_sin', 'month_cos',\n",
        "                'is_weekend', 'number_tx_exceeded', 'cash_withdrawal_exceeded',\n",
        "                'card_number_invalid', 'invalid_pin', 'invalid_cvv',\n",
        "                'rejected_accepted_ratio', 'remote_ratio', 'amount_transaction_ratio',\n",
        "                'avg_transaction_per_merchant', 'rolling_mean_7', 'rolling_max_7',\n",
        "                'rolling_min_7', 'rolling_sum_7', 'rolling_std_7', 'rolling_mean_14',\n",
        "                'rolling_max_14', 'rolling_min_14', 'rolling_sum_14', 'rolling_std_14',\n",
        "                'lag_1', 'lag_7', 'lag_14', 'day_of_month_cos', 'day_of_month_sin',\n",
        "                'mcc_6051_amount', 'mcc_4829_amount', 'small_merchant_amount',\n",
        "                'big_merchant_amount', 'new_merchant_amount', 'mcc_7995_amount',\n",
        "                'mcc_5999_amount', 'mcc_4722_amount', 'penalized_total_operation_amount',\n",
        "                'avg_pen_amount_per_card', 'avg_pen_amount_norm']\n",
        "    \n",
        "    X_train = df_train_balanced[features]\n",
        "    y_train = df_train_balanced[\"corrected_accepted_fraud_amount\"]\n",
        "    \n",
        "    X_test = df_test[features]\n",
        "    y_test = df_test[\"corrected_accepted_fraud_amount\"]\n",
        "    \n",
        "    print(f\"Number of features: {len(features)}\")\n",
        "    \n",
        "    # Hyperparameter tuning\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [100, 300],\n",
        "        \"max_depth\": [10, 20],\n",
        "        \"min_samples_split\": [2, 5],\n",
        "        \"min_samples_leaf\": [1, 2],\n",
        "        \"max_features\": ['sqrt', 'log2']\n",
        "    }\n",
        "    \n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "    rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "    grid_search = GridSearchCV(rf, param_grid, cv=3, scoring=\"neg_mean_absolute_error\", verbose=1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_rf = grid_search.best_estimator_\n",
        "    \n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = best_rf.predict(X_test)\n",
        "    \n",
        "    # Visualization\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    plt.plot(test_dates.values, y_test.values, label='Actual', linewidth=2)\n",
        "    plt.plot(test_dates.values, y_pred, label='Predicted', linewidth=2)\n",
        "    plt.title(\"Corrected Actual vs. Predicted Fraud Amount (Out-of-Sample)\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Fraud Amount Accepted\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Model evaluation\n",
        "    def penalized_mae_function(y_true, y_pred):\n",
        "        \"\"\"Custom penalized MAE function\"\"\"\n",
        "        return np.mean(np.abs(y_true - y_pred))\n",
        "    \n",
        "    if isinstance(y_pred, np.ndarray):\n",
        "        y_pred = pd.Series(y_pred, index=y_test.index)\n",
        "\n",
        "    print(f\"\\nEvaluation on {len(y_pred)} test days:\")\n",
        "\n",
        "    if len(y_pred) >= 1:\n",
        "        y1, p1 = y_test.iloc[:1], y_pred.iloc[:1]\n",
        "        print(f\"1-Day → MAE: {mean_absolute_error(y1, p1):.2f}, \"\n",
        "              f\"MAPE: {np.mean(np.abs((y1 - p1) / y1)) * 100:.2f}%, \"\n",
        "              f\"Penalized MAE: {penalized_mae_function(y1.values, p1.values):.2f}\")\n",
        "\n",
        "    if len(y_pred) >= 7:\n",
        "        y7, p7 = y_test.iloc[:7], y_pred.iloc[:7]\n",
        "        print(f\"7-Day → MAE: {mean_absolute_error(y7, p7):.2f}, \"\n",
        "              f\"MAPE: {np.mean(np.abs((y7 - p7) / y7)) * 100:.2f}%, \"\n",
        "              f\"Penalized MAE: {penalized_mae_function(y7.values, p7.values):.2f}\")\n",
        "\n",
        "    if len(y_pred) >= 30:\n",
        "        y30, p30 = y_test.iloc[:30], y_pred.iloc[:30]\n",
        "        print(f\"30-Day → MAE: {mean_absolute_error(y30, p30):.2f}, \"\n",
        "              f\"MAPE: {np.mean(np.abs((y30 - p30) / y30)) * 100:.2f}%, \"\n",
        "              f\"Penalized MAE: {penalized_mae_function(y30.values, p30.values):.2f}\")\n",
        "    \n",
        "    return best_rf, y_pred, test_dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Complete Pipeline Execution\n",
        "\n",
        "This section runs the complete pipeline from data loading to model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_complete_pipeline():\n",
        "    \"\"\"\n",
        "    Execute the complete fraud detection pipeline from data loading to model evaluation.\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (model, predictions, final_data) - Final model, predictions, and processed data\n",
        "    \"\"\"\n",
        "    print(\"🚀 Starting Fraud Detection Pipeline...\")\n",
        "    \n",
        "    # 1. Initialize Spark session\n",
        "    print(\"\\n📊 1. Initializing Spark session...\")\n",
        "    spark = initialize_spark_session()\n",
        "    \n",
        "    # 2. Load data\n",
        "    print(\"\\n📂 2. Loading data...\")\n",
        "    training, oos, card_status_full = load_data(spark)\n",
        "    \n",
        "    # Combine training and out-of-sample for feature engineering\n",
        "    df = training.union(oos)\n",
        "    print(f\"Combined dataset shape: {df.count()} rows\")\n",
        "    \n",
        "    # 3. Create target variables\n",
        "    print(\"\\n🎯 3. Creating target variables...\")\n",
        "    df = create_target_variable(df, training)\n",
        "    \n",
        "    # 4. Create time features\n",
        "    print(\"\\n⏰ 4. Creating time features...\")\n",
        "    df = create_time_features(df)\n",
        "    \n",
        "    # 5. Create rolling and lag features\n",
        "    print(\"\\n📊 5. Creating rolling and lag features...\")\n",
        "    df, rolling_stats_7, rolling_stats_14, lag1_df, lag7_df, lag14_df = create_rolling_features(df)\n",
        "    \n",
        "    # 6. Create merchant features\n",
        "    print(\"\\n🏪 6. Creating merchant features...\")\n",
        "    df = create_merchant_features(df, training)\n",
        "    \n",
        "    # 7. Create card features\n",
        "    print(\"\\n💳 7. Creating card features...\")\n",
        "    df = create_card_features(df, card_status_full)\n",
        "    \n",
        "    # 8. Daily aggregation\n",
        "    print(\"\\n📈 8. Aggregating features daily...\")\n",
        "    daily_data = aggregate_daily_features(df)\n",
        "    \n",
        "    # 9. Convert to Pandas and merge\n",
        "    print(\"\\n🔄 9. Converting to Pandas and merging features...\")\n",
        "    df_pd = convert_to_pandas(daily_data, rolling_stats_7, rolling_stats_14, lag1_df, lag7_df, lag14_df)\n",
        "    print(f\"Final feature set shape: {df_pd.shape}\")\n",
        "    \n",
        "    # 10. Create corrected target\n",
        "    print(\"\\n🔧 10. Creating corrected target variable...\")\n",
        "    df_pd = create_corrected_target(df_pd, training)\n",
        "    \n",
        "    # 11. Prepare modeling data\n",
        "    print(\"\\n📊 11. Preparing modeling data...\")\n",
        "    train_data, test_data = prepare_modeling_data(df_pd)\n",
        "    \n",
        "    # 12. Feature selection analysis\n",
        "    print(\"\\n🔍 12. Performing feature selection analysis...\")\n",
        "    mi_df = feature_selection_analysis(train_data)\n",
        "    \n",
        "    # 13. Train and evaluate model\n",
        "    print(\"\\n🤖 13. Training and evaluating model...\")\n",
        "    model, predictions, test_dates = train_and_evaluate_model(train_data, test_data)\n",
        "    \n",
        "    print(\"\\n✅ Pipeline completed successfully!\")\n",
        "    \n",
        "    return model, predictions, df_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the complete pipeline\n",
        "print(\"Starting the complete fraud detection pipeline...\")\n",
        "model, predictions, final_data = run_complete_pipeline()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
